{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final & working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE_final_project_extract (AWS Lambda functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following AWS Lambda function extracts data from the official data service of the city of Chicago and the chosen meteorological service by calling the respective APIs. It then saves the obtained data into an S3 bucket using a daily update trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def get_taxi_data(formatted_datetime: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieves taxi data for the given date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        formatted_datetime (str): The date in YYYY-MM-DD format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        List: A list of taxi trip data dictionaries.\n",
    "        Returns a fitting error message if problem occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    url_taxi = (\n",
    "        f\"https://data.cityofchicago.org/resource/ajtu-isnz.json?\"\n",
    "        f\"$where=trip_start_timestamp >= '{formatted_datetime}T00:00:00' \"\n",
    "        f\"AND trip_start_timestamp <= '{formatted_datetime}T23:59:59'&$limit=35000\"\n",
    "    )\n",
    "    \n",
    "    headers = {\"X-App-Token\": os.environ.get(\"CHICAGO_KEY\")}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url_taxi)\n",
    "        response.raise_for_status()\n",
    "        taxi_data = response.json()\n",
    "        print(taxi_data)\n",
    "        print(len(taxi_data))\n",
    "    \n",
    "    except requests.exceptions.RequestException as req_exc:\n",
    "        print(f\"This HTTP Request failed: {req_exc}\")\n",
    "    \n",
    "    except ValueError as v_e:\n",
    "        print(f\"The JSON's parsing has failed: {v_e}\")\n",
    "        \n",
    "    return taxi_data\n",
    "    \n",
    "    \n",
    "def get_weather_data(formatted_datetime: str) -> List:\n",
    "    \"\"\"\n",
    "    Fetches weather data from the Open-Meteo API for the given date and location.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        formatted_datetime (str): The date in YYYY-MM-DD format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        A dictionary containing weather data. \n",
    "        Returns a fitting error message if the request fails or an error occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    url_weather = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "    \n",
    "    params = {\n",
    "        \"latitude\": 41.85,\n",
    "        \"longitude\": -87.65,\n",
    "        \"start_date\": formatted_datetime,\n",
    "        \"end_date\": formatted_datetime,\n",
    "        \"hourly\": \"temperature_2m,wind_speed_10m,rain,precipitation\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url_weather, params=params)\n",
    "        response.raise_for_status()\n",
    "        weather_data = response.json()\n",
    "        print(weather_data)\n",
    "        print(len(weather_data))\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_e:\n",
    "        print(f\"HTTP error occurred: {http_e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Other error occured: {e}\")\n",
    "        \n",
    "    return weather_data\n",
    "    \n",
    "\n",
    "def upload_to_s3(data: Dict, folder_name: str, file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Uploads the given data to an S3 bucket.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        data (Dict): The data to be uploaded, either taxi or weather data.\n",
    "        folder_name (str): The name of the folder in the S3 bucket.\n",
    "        file_name (str): The name of the file to be saved in the S3 bucket.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        None. This function does not return anything.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If there is an error during the S3 upload process.\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket = \"egle.final-project-bucket\",\n",
    "            Key = f\"raw_data/to_processed/{folder_name}/{file_name}\",\n",
    "            Body = json.dumps(data)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to S3: {e}\")\n",
    "    \n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    current_datetime = datetime.now() - relativedelta(months = 2)\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    \n",
    "    taxi_data_api_call = get_taxi_data(formatted_datetime)\n",
    "    weather_data_api_call = get_weather_data(formatted_datetime)\n",
    "    \n",
    "    \n",
    "    taxi_file_name = f\"taxi_raw_{formatted_datetime}.json\"\n",
    "    weather_file_name = f\"weather_raw_{formatted_datetime}.json\"\n",
    "    \n",
    "    upload_to_s3(data = taxi_data_api_call,\n",
    "                 folder_name = \"taxi_data\",\n",
    "                 file_name = taxi_file_name\n",
    "    )\n",
    "    print(\"Taxi data has been uploaded!\")\n",
    "    \n",
    "    upload_to_s3(data = weather_data_api_call,\n",
    "                 folder_name = \"weather_data\",\n",
    "                 file_name = weather_file_name\n",
    "    )\n",
    "    print(\"Weather data has been uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE_final_project_transform-load (AWS Lambda function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code was created in an AWS Lambda environment to handle the Transform and Load portion of the ETL pipeline. The \"DE_final_project_extract\" function gathers and stores raw data versions in S3, which this code handles. It loads them into the relevant, independently structured S3 Bucket after processing them in accordance with the procedures, inputs, and output variables listed in each function.\n",
    "\n",
    "### The triggers that were built state that a new dataset is automatically processed each time the DE_final_project_extract daily trigger pulls one. The updated dimension tables and the prior states are kept in specific files, along with the original raw databases and the modified databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "\n",
    "def taxi_trips_transformations(taxi_trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform transformations with the taxi data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        taxi_trips (pd.DataFrame): The DataFrame holding the daily taxi trips.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame: The cleaned, transformed DataFrame holding the daily taxi trips.\n",
    "    \"\"\"\n",
    "    if not isinstance(taxi_trips, pd.DataFrame):\n",
    "        raise TypeError(\"taxi_trips is not a valid pandas DataFrame.\")\n",
    "    \n",
    "    taxi_trips.drop([\"pickup_census_tract\",\n",
    "                      \"dropoff_census_tract\",\n",
    "                      \"pickup_centroid_location\",\n",
    "                      \"dropoff_centroid_location\"],\n",
    "                      axis = 1,\n",
    "                      inplace = True)\n",
    "    \n",
    "    taxi_trips.dropna(inplace = True)\n",
    "\n",
    "    taxi_trips.rename(columns = {\"pickup_community_area\" : \"pickup_community_area_id\",\n",
    "                                 \"dropoff_community_area\" : \"dropoff_community_area_id\"},\n",
    "                                 inplace = True)\n",
    "    \n",
    "    taxi_trips[\"datetime_for_weather\"] = pd.to_datetime(taxi_trips[\"trip_start_timestamp\"]).dt.floor(\"H\")\n",
    "\n",
    "    return taxi_trips\n",
    "    \n",
    "################################################################################    \n",
    "\n",
    "def update_taxi_trips_with_master_data(taxi_trips: pd.DataFrame,\n",
    "                                       payment_type_master: pd.DataFrame,\n",
    "                                       company_master: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Update the taxi_trips DataFrame with the company_master and payment_type_master ids, and delete the string columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        taxi_trips (pd.DataFrame): The DataFrame with the daily taxi trips.\n",
    "        payment_type_master (pd.DataFrame):The payment type master table.\n",
    "        company_master (pd.DataFrame): The company master table.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame: The taxi trips data, with only payment_type_id and company_id, without company or payment_type values.\n",
    "    \"\"\"\n",
    "    taxi_trips_id = taxi_trips.merge(payment_type_master, \n",
    "                                     on = \"payment_type\")\n",
    "    \n",
    "    taxi_trips_id = taxi_trips_id.merge(company_master, \n",
    "                                        on = \"company\")\n",
    "    \n",
    "    taxi_trips_id.drop([\"payment_type\", \"company\"],\n",
    "                       axis = 1,\n",
    "                       inplace = True)\n",
    "    \n",
    "    return taxi_trips_id\n",
    "    \n",
    "################################################################################\n",
    "\n",
    "def update_master(taxi_trips: pd.DataFrame, \n",
    "                  master: pd.DataFrame, \n",
    "                  id_column: str, \n",
    "                  value_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extend the master DataFrame with new values if there are any.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        taxi_trips (pd.DataFrame): DataFrame holding the taxi data.\n",
    "        master (pd.DataFrame): DataFrame holding the master data.\n",
    "        id_column (str): The id column of the master DataFrame.\n",
    "        value_column (str): Name of the column in master_df containing the values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: The updated master data, if new values are in the taxi data, they will be loaded to it.\n",
    "    \"\"\"\n",
    "    max_id = master[id_column].max()\n",
    "\n",
    "    new_values_list = list(set(taxi_trips[value_column].values) - set(master[value_column].values))\n",
    "    new_values_df = pd.DataFrame({\n",
    "        id_column: range(max_id + 1, max_id + len(new_values_list) + 1),\n",
    "        value_column: new_values_list\n",
    "    })\n",
    "\n",
    "    updated_master = pd.concat([master, new_values_df], \n",
    "                               ignore_index = True)\n",
    "    \n",
    "    return updated_master\n",
    "    \n",
    "################################################################################\n",
    "\n",
    "def transform_weather_data(weather_data: json) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make transformations on the daily weather api response.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        weather_data (json): The daily weather data from the Open Meteo API\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame: A DataFrame representation of the data.\n",
    "    \"\"\"\n",
    "    weather_data_filtered = {\n",
    "        \"datetime\": weather_data[\"hourly\"][\"time\"],\n",
    "        \"temperature\": weather_data[\"hourly\"][\"temperature_2m\"],\n",
    "        \"wind_speed\": weather_data[\"hourly\"][\"wind_speed_10m\"],\n",
    "        \"rain\": weather_data[\"hourly\"][\"rain\"],\n",
    "        \"precipitation\": weather_data[\"hourly\"][\"precipitation\"]\n",
    "    }\n",
    "\n",
    "    weather_df = pd.DataFrame(weather_data_filtered)\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"datetime\"])\n",
    "\n",
    "    return weather_df\n",
    "    \n",
    "################################################################################\n",
    "\n",
    "def read_csv_from_s3(bucket: str, \n",
    "                     path: str, \n",
    "                     filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads a csv file from an S3 bucket.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        bucket (str): The bucket where the files are.\n",
    "        path (str): The folders to the file.\n",
    "        filename (str): Name of the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame: A DataFrame of the downloaded file.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    full_path = f\"{path}{filename}\"\n",
    "    obj = s3.get_object(Bucket = bucket, \n",
    "                        Key = full_path)\n",
    "    output_df = pd.read_csv(obj[\"Body\"])\n",
    "\n",
    "    return output_df\n",
    "    \n",
    "################################################################################\n",
    "\n",
    "def read_json_from_s3(bucket: str, \n",
    "                      path: str, \n",
    "                      filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    This function is an extra adjustment to replace the following lines of code with\n",
    "    a singular function call. It's able to fetch and load a json file from an S3 bucket.\n",
    "\n",
    "    This rows have been replaced by the read_json_from_s3:\n",
    "\n",
    "        response = s3.get_object(Bucket=bucket, \n",
    "                                 Key=taxi_trip_key)\n",
    "        content = response[\"Body\"]\n",
    "        taxi_trips_data_json = json.loads(content.read())\n",
    "        \n",
    "        \n",
    "        response = s3.get_object(Bucket = bucket, \n",
    "                                         Key = weather_key)\n",
    "        content = response[\"Body\"]\n",
    "        weather_data_json = json.loads(content.read())\n",
    "\n",
    "    Args:\n",
    "        bucket (str): name of the S3 bucket.\n",
    "        path (str): The path to the file in the bucket.\n",
    "        filename (str): The name of the file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The downloaded and loaded JSON data.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    full_path = f\"{path}{filename}\"\n",
    "    obj = s3.get_object(Bucket = bucket, \n",
    "                        Key = full_path)\n",
    "    \n",
    "    content = obj[\"Body\"].read()\n",
    "    data = json.loads(content)\n",
    "\n",
    "    return data\n",
    "    \n",
    "################################################################################\n",
    "\n",
    "def upload_dataframe_to_s3(dataframe: pd.DataFrame, \n",
    "                           bucket: str, \n",
    "                           path: str):\n",
    "    \"\"\"\n",
    "    Uploads a dataframe to the specified S3 path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataframe (pd.DataFrame):The dataframe to be uploaded.\n",
    "        bucket (str): Name of the S3 bucket where we want to store the files.\n",
    "        path (str)): Path within the bucket to upload the files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        This  function won't return anything.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    buffer = StringIO()\n",
    "\n",
    "    dataframe.to_csv(buffer, index = False)\n",
    "    df_content = buffer.getvalue()\n",
    "    s3.put_object(Bucket = bucket, \n",
    "                  Key = path, \n",
    "                  Body = df_content)\n",
    "                  \n",
    "################################################################################\n",
    "\n",
    "def upload_master_data_to_s3(bucket: str, \n",
    "                             path: str, \n",
    "                             file_type: str, \n",
    "                             dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Uploads master data (payment_type or company) to S3. Copies the previous version and creates the new one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        bucket (str): Name of the S3 bucket where we want to store the files.\n",
    "        path (str): Path within the bucket to upload the files.\n",
    "        file_type (str): Either \"company\" or \"payment_type\".\n",
    "        dataframe (pd.DataFrame): The dataframe to be uploaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        This function won't return anything.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    master_file_path = f\"{path}{file_type}_master.csv\"\n",
    "    previous_master_file_path = f\"{path}previous_version/{file_type}_master_previous_version.csv\"\n",
    "\n",
    "    s3.copy_object(Bucket = bucket, \n",
    "                   CopySource = {\"Bucket\": bucket, \"Key\": master_file_path}, \n",
    "                   Key = previous_master_file_path)\n",
    "    \n",
    "    upload_dataframe_to_s3(dataframe = dataframe, \n",
    "                           bucket = bucket, \n",
    "                           path = master_file_path)\n",
    "                           \n",
    "################################################################################\n",
    "\n",
    "def upload_and_move_file_on_s3(dataframe: pd.DataFrame, \n",
    "                               datetime_col: str, \n",
    "                               bucket: str, \n",
    "                               file_type: str, \n",
    "                               filename: str, \n",
    "                               source_path: str, \n",
    "                               target_path_raw: str, \n",
    "                               target_path_transformed: str):\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to S3 and then moves a file from the base folder to another.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataframe (pd.DataFrame):The DataFrame to be uploaded.\n",
    "        datetime_col (str): Datetime column name, which we derive the date for the filename.\n",
    "        bucket (str): Name of the S3 bucket.\n",
    "        file_type (str): \"weather\" or \"taxi\".\n",
    "        source_path (str): Source path within the bucket.\n",
    "        target_path_transformed (str): Target path within the bucket where the transformed data would go.\n",
    "        target_path_raw (str): Target path within the bucket where the raw data would go.\n",
    "        filename (str): Name of the file to be uploaded or moved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        This function won't return anything.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    formatted_date = dataframe[datetime_col].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    new_path_with_filename = f\"{target_path_transformed}{file_type}_{formatted_date}.csv\"\n",
    "\n",
    "    upload_dataframe_to_s3(dataframe = dataframe, \n",
    "                           bucket = bucket, \n",
    "                           path = new_path_with_filename)\n",
    "    \n",
    "    try:\n",
    "        s3.head_object(Bucket = bucket, Key = source_path)\n",
    "        s3.copy_object(Bucket = bucket, \n",
    "                       CopySource = {\"Bucket\": bucket, \"Key\": source_path}, \n",
    "                       Key = f\"{target_path_raw}{filename}\")\n",
    "        \n",
    "        s3.delete_object(Bucket = bucket, Key = source_path)\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        print(f\"Source path {source_path} does not exist.\")\n",
    "        raise\n",
    "    \n",
    "################################################################################\n",
    "\n",
    "################################  MAIN PART  ###################################\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Main function for AWS Lambda to handle taxi and weather data processing and uploading to S3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        event (dict): Event data passed by AWS Lambda.\n",
    "        context (object): Runtime information provided by AWS Lambda.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = \"egle.final-project-bucket\"\n",
    "\n",
    "    raw_taxi_trips_folder = \"raw_data/to_processed/taxi_data/\"\n",
    "    raw_weather_folder = \"raw_data/to_processed/weather_data/\"\n",
    "    target_taxi_trips_folder = \"raw_data/processed/taxi_data/\"\n",
    "    target_weather_folder = \"raw_data/processed/weather_data/\"\n",
    "    transformed_taxi_trips_folder = \"transformed_data/taxi_trips/\"\n",
    "    transformed_weather_folder = \"transformed_data/weather/\"\n",
    "    payment_type_master_folder = \"transformed_data/payment_type/\"\n",
    "    company_master_folder = \"transformed_data/company/\"\n",
    "    payment_type_master_file_name = \"payment_type_master.csv\"\n",
    "    company_master_file_name = \"company_master.csv\"\n",
    "\n",
    "\n",
    "    payment_type_master = read_csv_from_s3(bucket = bucket, \n",
    "                                           path = payment_type_master_folder, \n",
    "                                           filename = payment_type_master_file_name)\n",
    "    \n",
    "    company_master = read_csv_from_s3(bucket = bucket, \n",
    "                                      path = company_master_folder, \n",
    "                                      filename = company_master_file_name)\n",
    "\n",
    "    for file in s3.list_objects(Bucket = bucket, Prefix = raw_taxi_trips_folder)[\"Contents\"]:\n",
    "        taxi_trip_key = file[\"Key\"]\n",
    "        if taxi_trip_key.split(\"/\")[-1].strip() != \"\":\n",
    "            if taxi_trip_key.split(\".\")[-1] == \"json\":\n",
    "                filename = taxi_trip_key.split(\"/\")[-1]\n",
    "                taxi_trips_data_json = read_json_from_s3(bucket, \n",
    "                                                         raw_taxi_trips_folder, \n",
    "                                                         filename)\n",
    "                taxi_trips_data_raw = pd.DataFrame(taxi_trips_data_json)\n",
    "                taxi_trips_transformed = taxi_trips_transformations(taxi_trips_data_raw)\n",
    "\n",
    "                company_master_updated = update_master(taxi_trips_transformed, \n",
    "                                                       company_master, \n",
    "                                                       \"company_id\", \n",
    "                                                       \"company\")\n",
    "                \n",
    "                payment_type_master_updated = update_master(taxi_trips_transformed, \n",
    "                                                            payment_type_master, \n",
    "                                                            \"payment_type_id\", \n",
    "                                                            \"payment_type\")\n",
    "                \n",
    "                taxi_trips = update_taxi_trips_with_master_data(taxi_trips_transformed, \n",
    "                                                                payment_type_master_updated, \n",
    "                                                                company_master_updated)\n",
    "                \n",
    "                upload_and_move_file_on_s3(\n",
    "                    dataframe = taxi_trips,\n",
    "                    datetime_col = \"datetime_for_weather\",\n",
    "                    bucket = bucket,\n",
    "                    file_type = \"taxi\",\n",
    "                    filename = filename,\n",
    "                    source_path = taxi_trip_key,\n",
    "                    target_path_raw = target_taxi_trips_folder,\n",
    "                    target_path_transformed = transformed_taxi_trips_folder\n",
    "                )\n",
    "                print(\"taxi_trips is uploaded and moved.\")\n",
    "\n",
    "                upload_master_data_to_s3(bucket = bucket, \n",
    "                                         path = payment_type_master_folder, \n",
    "                                         file_type = \"payment_type\", \n",
    "                                         dataframe = payment_type_master_updated)\n",
    "                \n",
    "                upload_master_data_to_s3(bucket = bucket, \n",
    "                                         path = company_master_folder, \n",
    "                                         file_type = \"company\", \n",
    "                                         dataframe = company_master_updated)\n",
    "                \n",
    "                print(\"company_master has been updated.\")\n",
    "\n",
    "    for file in s3.list_objects(Bucket = bucket, Prefix = raw_weather_folder)[\"Contents\"]:\n",
    "        weather_key = file[\"Key\"]\n",
    "        if weather_key.split(\"/\")[-1].strip() != \"\":\n",
    "            if weather_key.split(\".\")[-1] == \"json\":\n",
    "                filename = weather_key.split(\"/\")[-1]\n",
    "                weather_data_json = read_json_from_s3(bucket, \n",
    "                                                      raw_weather_folder, \n",
    "                                                      filename)\n",
    "                weather_data = transform_weather_data(weather_data_json)\n",
    "                \n",
    "                upload_and_move_file_on_s3(\n",
    "                    dataframe = weather_data,\n",
    "                    datetime_col = \"datetime\",\n",
    "                    bucket = bucket,\n",
    "                    file_type = \"weather\",\n",
    "                    filename = filename,\n",
    "                    source_path = weather_key,\n",
    "                    target_path_raw = target_weather_folder,\n",
    "                    target_path_transformed = transformed_weather_folder\n",
    "                )\n",
    "                print(\"weather is uploaded and moved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
